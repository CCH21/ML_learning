# 实在写不出来

百尺竿头了结果出错 可能下次会改吧

源代码是01_test 但是我认为用矩阵来做更方便
大体的思想是了解了 但是撸不出代码 各位若有想法学习的话可以参考这个代码的备注和源码

总结一下决策树的总体思想：

1、 决策树的定义就是通过一系列对特征的判断来形成分支条件，最终逐渐形成一棵树的样子。我们可以理解成一系列的if-else语句。

2、 决策树中存在一个步骤来选择最适合做分割的变量，即香农熵。熵是定义信息混乱程度的指标，熵越大则越混乱，熵越小则越整齐。所以决策树在选择特征做分割的时候会遍历所有特征逐一剔除，从而计算剩余矩阵的熵，选择熵最小的剩余矩阵来保留，即选择使得剩余矩阵熵最小的特征剔除。

3、 有了特征的选择方法后就是对矩阵特征逐一删除，从而得到越来越小的矩阵。这也是整个树建立的过程。

| 下面用一个例子来解释
| ---



       nosurfacing  flippers label
    0            1         1   yes
    1            1         1   yes
    2            1         0    no
    3            0         1    no
    4            0         1    no
    # 这是原矩阵的香农熵 可以看到此时熵很大 所以很无序
    0.9709505944546686



	# 下面两个小矩阵是针对第一列遍历0,1两种结果取得的子矩阵 split_data_set()函数的作用是除去index列和index列上不等于value的行
       flippers label
    3         1    no
    4         1    no
       flippers label
    0         1   yes
    1         1   yes
    2         0    no
    # 可以看到两个小矩阵的香农熵之和为0.55 小了很多
    0.5509775004326937


	# 这是对第二列遍历0,1 最终打印出熵
	   nosurfacing label
    2            1    no
       nosurfacing label
    0            1   yes
    1            1   yes
    3            0    no
    4            0    no
    # 同理，这两个矩阵香农熵0.8 比按照第一列的value=0分割混乱（熵大）
    0.8

**注：信息增益的算法就是原矩阵的香农熵(0.97)-分割后的香农熵(越小则增益越大),增益作为最终选择特征的标准来做判断**

经过上面的计算，可以选出第一次分割的特征是第一个特征。然后用第一个特征来构建树的根节点。即

	if nosurfacing == 1:
		pass
	else:
		pass